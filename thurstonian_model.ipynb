{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Thurstonian Utility Model\n",
    "\n",
    "## Introduction\n",
    "A **Thurstonian utility model** is a framework used to explain how people make choices when comparing different options. It originates from a psychological theory developed by L. L. Thurstone, which explores how individuals perceive differences between items.\n",
    "\n",
    "---\n",
    "\n",
    "## Core Concept\n",
    "Imagine you have two kinds of food in front of you. You take a bite of each and decide which one you prefer. However, your decision isn't always 100% clear-cut—sometimes the difference between the two is small, and you might even pick differently if you tried again.\n",
    "\n",
    "The Thurstonian model assumes that when comparing options, your brain assigns an **internal value** (or utility) to each one. This value, however, includes some randomness or uncertainty. As a result:\n",
    "- Even if Food `A` is slightly better on average, you might sometimes pick Food `B due to this inherent variability in perception.\n",
    "\n",
    "---\n",
    "\n",
    "## Key Ideas\n",
    "\n",
    "### 1. Comparisons, Not Absolute Scores\n",
    "- People don't judge things on a fixed scale; instead, they evaluate options relative to each other.\n",
    "- The focus is on pairwise comparisons rather than assigning absolute ratings.\n",
    "\n",
    "### 2. Randomness (Noise)\n",
    "- Perception is not deterministic. There is always some level of uncertainty or \"noise\" in how we evaluate options.\n",
    "- This randomness accounts for inconsistencies in decision-making.\n",
    "\n",
    "### 3. Probability of Preference\n",
    "- The greater the difference between two options, the more likely you are to consistently choose the better one.\n",
    "- When options are very similar, the probability of choosing either becomes closer to 50%.\n",
    "\n",
    "---\n",
    "\n",
    "## Applications\n",
    "The Thurstonian model is widely used in various fields, including:\n",
    "- **Psychology**: To study human perception and decision-making processes.\n",
    "- **Marketing**: To predict consumer preferences and optimize product offerings.\n",
    "- **AI**: To design algorithms that mimic human-like decision-making in scenarios involving uncertain or noisy data.\n",
    "\n",
    "This model provides a robust framework for understanding and predicting how people choose between similar alternatives!\n",
    "\n",
    "---\n",
    "\n",
    "## Thurstonian Utility Model\n",
    "\n",
    "In a **Thurstonian utility model**, a **Gaussian distribution** represents the uncertainty or variability in how people perceive the value of each option. Here's how it works:\n",
    "\n",
    "### 1. Each Option Gets a Gaussian (Normal) Distribution\n",
    "- Imagine you are comparing a **Burger** and an **Apple**.\n",
    "- Each food item has an **underlying \"true value\"** (or utility), but you don’t perceive it perfectly. Instead, your brain assigns a utility that varies slightly each time due to randomness (e.g., mood, hunger, environment).\n",
    "- This variation is modeled using a **Gaussian distribution** with:\n",
    "  - A **mean** ($\\mu$) representing the average perceived utility of that option.\n",
    "  - A **standard deviation** ($\\sigma$) representing the amount of randomness or noise in perception.\n",
    "\n",
    "Mathematically, we say:\n",
    "\n",
    "$$\n",
    "U_{\\text{Burger}} \\sim \\mathcal{N}(\\mu_{\\text{Burger}}, \\sigma_{\\text{Burger}}^2)\n",
    "$$\n",
    "\n",
    "$$\n",
    "U_{\\text{Apple}} \\sim \\mathcal{N}(\\mu_{\\text{Apple}}, \\sigma_{\\text{Apple}}^2)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $U_{\\text{Burger}}$ and $U_{\\text{Apple}}$ are the **perceived utilities**.\n",
    "- $\\mathcal{N}(\\mu, \\sigma^2)$ is a normal distribution.\n",
    "- The means ($\\mu_{\\text{Burger}}, \\mu_{\\text{Apple}}$) are the \"true\" utilities.\n",
    "- The standard deviations ($\\sigma_{\\text{Burger}}, \\sigma_{\\text{Apple}}$) represent uncertainty in perception.\n",
    "\n",
    "### 2. Making a Choice: Pairwise Comparison\n",
    "- When comparing the two options, you **draw** a sample utility from each distribution.\n",
    "- You then choose the option with the higher sampled utility.\n",
    "- This means if $U_{\\text{Burger}} > U_{\\text{Apple}}$, you choose **Burger**; otherwise, you choose **Apple**.\n",
    "- The probability of choosing **Burger** depends on how much its utility distribution overlaps with **Apple**'s.\n",
    "\n",
    "### 3. Probability of Preference\n",
    "- If the distributions overlap a lot, your choice is more uncertain.\n",
    "- If they are far apart, you're almost always going to pick the better one.\n",
    "- Mathematically, the probability of choosing **Burger** over **Apple** is given by:\n",
    "\n",
    "$$\n",
    "P(\\text{Burger is chosen}) = P(U_{\\text{Burger}} > U_{\\text{Apple}})\n",
    "$$\n",
    "\n",
    "This can be computed using the **cumulative distribution function (CDF)** of a normal distribution.\n",
    "\n",
    "---\n",
    "\n",
    "## Example in Action\n",
    "Imagine you rate foods:\n",
    "- **Burger:** $\\mu_{\\text{Burger}} = 7$, $\\sigma_{\\text{Burger}} = 1.5$\n",
    "- **Apple:** $\\mu_{\\text{Apple}} = 6.5$, $\\sigma_{\\text{Apple}} = 2$\n",
    "\n",
    "Even though **Burger is slightly better on average**, the overlap in their distributions means you won't always pick it. Some days, the Apple might taste better due to small variations in perception!\n",
    "\n",
    "This is why the Thurstonian model explains **probabilistic choices** rather than strict, fixed preferences. It's useful in psychology, marketing, and machine learning for modeling **how people make uncertain decisions**."
   ],
   "id": "c047da5d6c032eed"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Code example with 3 choices\n",
    "\n",
    "We consider an Apple, Burger and Chocolate"
   ],
   "id": "7f98ae41f924d96c"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-13T16:06:05.562820Z",
     "start_time": "2025-02-13T16:06:05.514481Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# Example data: (Option A, Option B, Chosen Option)\n",
    "# A: Apple\n",
    "# B: Burger\n",
    "# C: Chocolate\n",
    "data = [\n",
    "    (\"B\", \"A\", \"B\"),  # Burger preferred over Apple\n",
    "    (\"A\", \"C\", \"C\"),  # Chocolate preferred over Apple\n",
    "    (\"B\", \"C\", \"B\"),  # Burger preferred over Chocolate\n",
    "    (\"A\", \"B\", \"A\"),  # Apple preferred over Burger -----> \"Inconsistency\", hence the probabilistic approach\n",
    "    (\"C\", \"A\", \"C\"),  # Chocolate preferred over Apple\n",
    "    (\"C\", \"B\", \"B\"),  # Burger preferred over Chocolate\n",
    "    (\"B\", \"A\", \"B\"),  # Burger preferred over Apple\n",
    "    (\"A\", \"C\", \"A\"),  # Apple preferred over Chocolate -----> \"Inconsistency\", hence the probabilistic approach\n",
    "    (\"C\", \"B\", \"C\"),  # Chocolate preferred over Burger\n",
    "    (\"A\", \"B\", \"B\"),  # Burger preferred over Apple\n",
    "]\n",
    "\n",
    "# Step 1: Initialize parameters\n",
    "options = list(set([item for sublist in data for item in sublist[:2]]))\n",
    "if len(options) < 2:\n",
    "    raise ValueError(\"Insufficient unique options in the data\")\n",
    "\n",
    "mu = {opt: 0 for opt in options}  # Initialize means to 0\n",
    "sigma = {opt: 1.0 for opt in options}  # Initial variance is 1\n",
    "\n",
    "# Step 2: Define likelihood function\n",
    "def neg_log_likelihood(params):\n",
    "    for i, opt in enumerate(options):\n",
    "        mu[opt] = params[i]\n",
    "        sigma[opt] = np.exp(params[i + len(options)])  # Ensure positive sigma\n",
    "\n",
    "    likelihood = 0\n",
    "    epsilon = 1e-10  # Small constant for numerical stability\n",
    "    for A, B, chosen in data:\n",
    "        # Probability estimation (classical formula)\n",
    "        mu_diff = mu[A] - mu[B]\n",
    "        sigma_sum = np.sqrt(sigma[A]**2 + sigma[B]**2)\n",
    "        prob_A = norm.cdf(mu_diff / (sigma_sum + epsilon))\n",
    "        # Binary log-likelihood\n",
    "        likelihood += np.log(prob_A if chosen == A else 1 - prob_A + epsilon)\n",
    "    return -likelihood\n",
    "\n",
    "# Step 3: Optimize\n",
    "init_params = [0] * len(options) + [0] * len(options)  # Initialize mu and log(sigma)\n",
    "bounds = [(-10, 10)] * len(options) + [(-10, 2)] * len(options)  # Bounds for mu and log(sigma)\n",
    "result = minimize(neg_log_likelihood, init_params, method=\"L-BFGS-B\", bounds=bounds)\n",
    "\n",
    "# Extract results\n",
    "mu_estimates = result.x[:len(options)]\n",
    "sigma_estimates = np.exp(result.x[len(options):])\n",
    "\n",
    "# Print estimated parameters\n",
    "print(\"Estimated parameters:\")\n",
    "for i, opt in enumerate(options):\n",
    "    print(f\"{opt}: mu = {mu_estimates[i]:.2f}, sigma = {sigma_estimates[i]:.2f}\")\n",
    "\n",
    "# Optional: Calculate and print confidence intervals\n",
    "# This would require computing the Hessian matrix at the optimum\n",
    "\n",
    "print(f\"\\nLog-likelihood: {-result.fun:.2f}\")\n",
    "print(f\"Converged: {result.success}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated parameters:\n",
      "A: mu = -0.57, sigma = 1.20\n",
      "C: mu = 0.00, sigma = 0.57\n",
      "B: mu = 0.57, sigma = 1.20\n",
      "\n",
      "Log-likelihood: -6.07\n",
      "Converged: True\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We can see some \"inconsistencies\" that can be represented with this model. The user generally chooses B > A, but there is an instance of A > B\n",
    "\n",
    "## Summary of Results:\n",
    "\n",
    "1. Preference Ranking: Burger > Chocolate > Apple\n",
    "   (Based on estimated means: B: 0.57, C: 0.00, A: -0.57)\n",
    "\n",
    "2. Preference Consistency:\n",
    "   - Chocolate shows the most consistent preferences (σ = 0.57)\n",
    "   - Burger and Apple have equal, higher variability (σ = 1.20 each)\n",
    "\n",
    "3. Key Observations:\n",
    "   - Burger is most preferred but with high variability\n",
    "   - Chocolate is moderately preferred with the most consistent ratings\n",
    "   - Apple is least preferred, also with high variability"
   ],
   "id": "7e5456cc6ad626b1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# How to get the confidence of preferences?\n",
    "\n",
    "I was asking Sonnet 3.5 because I was not 100% sure. I think we could use the data to get **frenquencies** but wanted to double check using the model"
   ],
   "id": "b79e2c00fd2ca8ac"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T16:06:05.569314Z",
     "start_time": "2025-02-13T16:06:05.567040Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "mu_A, mu_B = -0.57, 0.57\n",
    "sigma_A, sigma_B = 1.20, 1.20\n",
    "\n",
    "prob_A_greater_B = norm.cdf((mu_A - mu_B) / np.sqrt(sigma_A**2 + sigma_B**2))\n",
    "print(f\"Probability of A > B: {prob_A_greater_B:.4f}\")"
   ],
   "id": "7e1d5173d3345ec1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of A > B: 0.2509\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T16:06:05.642863Z",
     "start_time": "2025-02-13T16:06:05.640195Z"
    }
   },
   "cell_type": "code",
   "source": [
    "A_wins = sum(1 for a, b, chosen in data if (a == 'A' and b == 'B' and chosen == 'A') or (a == 'B' and b == 'A' and chosen == 'A'))\n",
    "total_AB_comparisons = sum(1 for a, b, _ in data if set([a, b]) == set(['A', 'B']))\n",
    "\n",
    "raw_prob_A_greater_B = A_wins / total_AB_comparisons\n",
    "print(f\"Raw frequency of A > B: {raw_prob_A_greater_B:.4f}\")"
   ],
   "id": "e20ac23520200a5d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw frequency of A > B: 0.2500\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## \"Emergence\" of utilities\n",
    "\n",
    "We use cross-entropy here to compare how similar is the model's probability to the data. To have a really low cross-entropy the data should not have any \"inconsistencies\".\n",
    "\n"
   ],
   "id": "cb918611d18b7385"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T16:06:05.716009Z",
     "start_time": "2025-02-13T16:06:05.712216Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "\n",
    "def calculate_pairwise_prob(mu_X, mu_Y, sigma_X, sigma_Y):\n",
    "    return norm.cdf((mu_X - mu_Y) / np.sqrt(sigma_X**2 + sigma_Y**2))\n",
    "\n",
    "def calculate_cross_entropy(data, mu, sigma):\n",
    "    cross_entropy = 0\n",
    "    for X, Y, chosen in data:\n",
    "        p_X_over_Y = calculate_pairwise_prob(mu[X], mu[Y], sigma[X], sigma[Y])\n",
    "\n",
    "        # Observed choice: 1 if X was chosen, 0 if Y was chosen\n",
    "        y = 1 if chosen == X else 0\n",
    "\n",
    "        # Cross-entropy formula\n",
    "        cross_entropy -= (y * np.log(p_X_over_Y) + (1-y) * np.log(1 - p_X_over_Y))\n",
    "\n",
    "    return cross_entropy / len(data)\n",
    "\n",
    "# Using the fitted parameters from before\n",
    "mu = {'A': -0.57, 'B': 0.57, 'C': 0.00}\n",
    "sigma = {'A': 1.20, 'B': 1.20, 'C': 0.57}\n",
    "\n",
    "cross_entropy = calculate_cross_entropy(data, mu, sigma)\n",
    "print(f\"Cross-entropy: {cross_entropy:.4f}\")"
   ],
   "id": "bdd5a40d404674d3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-entropy: 0.6068\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### What does it mean to have a low cross-netropy here?\n",
    "\n",
    "A low cross-entropy in the context of the Thurstonian model has several important implications:\n",
    "\n",
    "1. **Good Model Fit**: The model's predictions align closely with observed choices, indicating it captures the underlying preference structure well.\n",
    "\n",
    "2. **Accurate Predictions**: The model predicts pairwise choices with high accuracy, closely matching actual observed choice frequencies.\n",
    "\n",
    "3. **Consistent Preferences**: Suggests that preferences in the data are relatively consistent and well-captured by the model's assumptions (e.g., normally distributed utilities).\n",
    "\n",
    "4. **Useful for Decision Making**: The model can more reliably predict preferences in new situations or for items not directly compared in the original dataset.\n",
    "\n",
    "5. **Effective Parameter Estimation**: Indicates that estimated parameters (μ and σ for each item) likely represent true underlying preferences well.\n",
    "\n",
    "6. **Minimal Information Loss**: Less additional information is needed to explain observed choices beyond what the model provides.\n",
    "\n",
    "7. **Potential for Generalization**: The model is more likely (though not guaranteed) to generalize well to new, unseen data.\n",
    "\n",
    "In simple terms, low cross-entropy means the model is doing a good job of \"understanding\" and representing the preferences expressed in the choice data. It's akin to having a friend who can accurately predict your food choices because they have a good grasp of your tastes.\n",
    "\n",
    "**Note**: While low cross-entropy is generally desirable, it should be interpreted in context. Extremely low values might indicate overfitting, especially if the cross-entropy on a separate validation set is much higher. The relative value of cross-entropy (comparing across models or datasets) is often more informative than its absolute value."
   ],
   "id": "b3a70330ed91d79b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
